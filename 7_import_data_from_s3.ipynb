{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Required Packages (Add under Advanced Options > Packages):\n",
        "\n",
        "'''\n",
        "org.apache.hadoop:hadoop-aws:3.3.2\n",
        "org.apache.spark:spark-hadoop-cloud_2.12:3.5.1\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "iru-ISCgX8jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWPcpqmoUnAz"
      },
      "outputs": [],
      "source": [
        "#Configure Your AWS Credentials\n",
        "\n",
        "'''\n",
        "spark.conf.set(\"fs.s3a.access.key\", \"<insert value>\")\n",
        "spark.conf.set(\"fs.s3a.secret.key\", \"<insert value>\")\n",
        "spark.conf.set(\"fs.s3a.region\", \"<insert value>\")\n",
        "spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
        "spark.conf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
        "\n",
        "'''\n",
        "\n",
        "# Alternatively, you can set these key-value pairs in the notebook in:\n",
        "# Advanced Options > Configs\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example from a public S3 bucket (no credentials required)\n",
        "\n",
        "s3_path = \"s3a://ursa-labs-taxi-data/2019/01/data.parquet\"\n",
        "\n",
        "df = spark.read.parquet(s3_path)\n",
        "\n",
        "df.show(2)"
      ],
      "metadata": {
        "id": "h718PynqWa7a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
