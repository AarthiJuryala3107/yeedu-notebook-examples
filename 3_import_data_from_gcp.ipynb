{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Required Packages (Add under Advanced Options > Packages):\n",
        "\n",
        "'''\n",
        "org.apache.spark:spark-hadoop-cloud_2.12:3.5.1\n",
        "com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.25.2\n",
        "'''"
      ],
      "metadata": {
        "id": "iru-ISCgX8jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "wHiC4_qtAc0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWPcpqmoUnAz"
      },
      "outputs": [],
      "source": [
        "# Configure Your GCP Secrets (Add under Secrets -> Workspaces -> New Secret -> Secret Type = Environment Variable)\n",
        "\n",
        "private_key=os.getenv('gcp_pvtkey')\n",
        "private_key_id=os.getenv('gcp_pvtkeyid')\n",
        "\n",
        "spark.conf.set(\"google.cloud.auth.service.account.private.key\", private_key)\n",
        "spark.conf.set(\"google.cloud.auth.service.account.private.key.id\", private_key_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the rest of the Credentials\n",
        "\n",
        "spark.conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
        "spark.conf.set(\"google.cloud.auth.service.account.email\", \"gratis-bucket-sa@modak-nabu.iam.gserviceaccount.com\")\n",
        "spark.conf.set(\"fs.gs.project.id\", \"modak-nabu\")\n",
        "spark.conf.set(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
        "spark.conf.set(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
        "\n",
        "# Alternatively, you can set these key-value pairs in the notebook in:\n",
        "# Advanced Options > Configs\n"
      ],
      "metadata": {
        "id": "5Pd2le55AjUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing and Reading Data from GCP\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Cathy\", 27)]\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True)\n",
        "])\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Define GCP path\n",
        "output_path = \"gs://<insert-adls-path>\"\n",
        "\n",
        "# Write DataFrame to GCP\n",
        "df.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "# Read DataFrame from GCP\n",
        "df_read = spark.read.parquet(output_path)\n",
        "df_read.show()"
      ],
      "metadata": {
        "id": "YqfqXEV2Aw95"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}