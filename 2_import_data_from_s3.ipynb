{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Required Packages (Add under Advanced Options > Packages):\n",
        "\n",
        "'''\n",
        "org.apache.hadoop:hadoop-aws:3.3.2\n",
        "org.apache.spark:spark-hadoop-cloud_2.12:3.5.1\n",
        "'''"
      ],
      "metadata": {
        "id": "iru-ISCgX8jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "wHiC4_qtAc0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWPcpqmoUnAz"
      },
      "outputs": [],
      "source": [
        "# Configure Your AWS Secrets (Add under Secrets -> Workspaces -> New Secret -> Secret Type = Environment Variable)\n",
        "\n",
        "access_key=os.getenv('s3_accesskey')\n",
        "secret_key=os.getenv('s3_secretkey')\n",
        "\n",
        "spark.conf.set(\"fs.s3a.access.key\", access_key)\n",
        "spark.conf.set(\"fs.s3a.secret.key\", secret_key)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the rest of the Credentials\n",
        "\n",
        "spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
        "spark.conf.set(\"fs.s3a.region\", \"us-west-2\")\n",
        "spark.conf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
        "\n",
        "# Alternatively, you can set these key-value pairs in the notebook in:\n",
        "# Advanced Options > Configs"
      ],
      "metadata": {
        "id": "5Pd2le55AjUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for Importing data from a public S3 bucket (no credentials required)\n",
        "\n",
        "s3_path = \"s3a://ursa-labs-taxi-data/2019/01/data.parquet\"\n",
        "df = spark.read.parquet(s3_path)\n",
        "df.show(2)"
      ],
      "metadata": {
        "id": "h718PynqWa7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing and Reading Data from S3\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Cathy\", 27)]\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True)\n",
        "])\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Define ADLS path\n",
        "output_path = \"s3a://<insert-s3-path>\"\n",
        "\n",
        "# Write DataFrame to S3\n",
        "df.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "# Read DataFrame from S3\n",
        "df_read = spark.read.parquet(output_path)\n",
        "df_read.show()"
      ],
      "metadata": {
        "id": "YqfqXEV2Aw95"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}